---
# Kubernetes node configuration
# Do not change profile_name, configured_nic and configured_arch here !!!
# To generate vars for different profile/architecture use make command
# generated for profile and arch:
profile_name: {{ name }}
configured_arch: {{ arch }}
configured_nic: {{ nic }}

{% if sriov_operator in ['on', 'optional'] or sriov_network_dp in ['on', 'optional'] or qat in ['on', 'optional'] or dsa in ['on', 'optional'] -%}
# Enable IOMMU (required for SR-IOV networking and QAT)
iommu_enabled: {% if (sriov_operator == 'on' or sriov_network_dp == 'on' or qat == 'on' or dsa == 'on' or dlb == 'on') and on_vms != 'on' %}true{% else %}false{% endif %}
{% endif %}
{% if name not in ['ifpd'] %}
# dataplane interface configuration list
dataplane_interfaces: []
{%- if on_vms == 'on' %}
#  - bus_info: "06:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "igb_uio"
#  - bus_info: "07:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "iavf"
#  - bus_info: "08:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "iavf"
#  - bus_info: "09:00.0"             # pci bus info
#    pf_driver: iavf              # driver inside VM
#    sriov_numvfs: 0
#    default_vf_driver: "igb_uio"
{%- else %}
#  - bus_info: "18:00.0"                    # pci bus info
#    pf_driver: {% if nic == 'cvl' %}ice{% else %}i40e{% endif %}                         # PF driver, "i40e", "ice"
{%- if ddp in ['on', 'optional'] %}
#    ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.37.0.pkg"{% else %}gtp.pkgo{% endif %}  # DDP package name to be loaded into the NIC
                                            # For i40e(XV710-*) allowable ddp values are: "ecpri.pkg", "esp-ah.pkg", "ppp-oe-ol2tpv2.pkgo", "mplsogreudp.pkg" and "gtp.pkgo", replace as required
                                            # For ice(E810-*) allowable ddp values are: ice_comms-1.3.[17,20,22,24,28,30,31,35].0.pkg  such as "ice_comms-1.3.37.0.pkg", replace as required
                                            # ddp_profile must be defined for first port of each network device. bifurcated cards will appear as unique devices.
{% endif %}
{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
#    flow_configuration: {% if intel_ethernet_operator.flow_config == 'on' and nic == "cvl" %}true{% else %}false{% endif %}              # Flow Configuration # NOTE: this option is for Intel E810 Series NICs and requires Intel Ethernet Operator and Flow Config to be enabled in group vars.
                                            # with Flow Configuration enabled the first VF (VF0) will be reserved for Flow Configuration and the rest of VFs will be indexed starting from 1.
{% endif %}
#    default_vf_driver: "iavf"              # default driver to be used with VFs if specific driver is not defined in the "sriov_vfs" section
#    sriov_numvfs: 6                        # total number of VFs to create including VFs listed in the "sriov_vfs" section.
                                            # If total number of VFs listed in the "sriov_vfs" section is greater than "sriov_numvfs" then excessive entities will be ignored.
                                            # VF's name should follow scheme: <arbitrary_vf_name>_<zero_started_index_of_vf>
                                            # If index in the VF's name is greater than "sriov_numfs - 1" such VF will be ignored.
{%- if minio in ['on', 'optional'] %}
#    minio_vf: true
{% endif %}
#    sriov_vfs:                             # list of VFs to create on this PF with specific driver
#      vf_00: "vfio-pci"                    # VF driver to be attached to this VF under this PF. Options: "iavf", "vfio-pci", "igb_uio"
#      vf_05: "vfio-pci"

#  - bus_info: "18:00.1"
#    pf_driver: {% if nic == 'cvl' %}ice{% else %}i40e{% endif %}
{%- if ddp in ['on', 'optional'] %}
#    ddp_profile: {% if nic == 'cvl' %}"ice_comms-1.3.37.0.pkg"{% else %}gtp.pkgo{% endif %}
{%- endif %}
#    default_vf_driver: "vfio-pci"
{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
#    flow_configuration: {% if intel_ethernet_operator.flow_config == 'on' and nic == "cvl" %}true{% else %}false{% endif %}
{% endif %}
#    sriov_numvfs: 4
{%- if minio in ['on', 'optional'] %}
#    minio_vf: true
{% endif %}
#    sriov_vfs: {}                   # no VFs with specific driver on this PF or "sriov_vfs" can be omitted for convenience
{% endif %}
{%- if nic_drivers in ['on', 'optional'] %}
# Set to 'true' to update i40e, ice and iavf kernel modules
update_nic_drivers: {% if nic_drivers == 'on' %}true{% else %}false{% endif %}
#i40e_driver_version: "2.20.12" # Downgrading i40e drivers is not recommended due to the possible consequences. Users should update and proceed at their own risk.
#i40e_driver_checksum: "sha1:a24f0c5512af31c68cd90667d5822121780d5487" # update checksum per required i40e drivers version
#ice_driver_version: "1.9.11"    # Downgrading ice drivers is not recommended due to the possible consequences. Users should update and proceed at their own risk.
#ice_driver_checksum: "sha1:f05e2322a66de5d4019e7aa6141a109bb419dda4"  # update checksum per required ice drivers version
#iavf_driver_version: "4.5.3" # Downgrading iavf drivers is not recommended due to the possible consequences. Users should update and proceed at their own risk.
#iavf_driver_checksum: "sha1:76b3a7dec392e559dea6112fa55f5614857cff2a" # update checksum per required iavf drivers version
{% endif %}
# Set 'true' to upgrade / downgrade NIC firmware. FW upgrade / downgrade will be executed on all NICs listed in "dataplane_interfaces[*].bus_info".
update_nic_firmware: false # Note: downgrading FW is not recommended, users should proceed at their own risk.
{%- if nic == 'fvl' %}
#nvmupdate: []  # remove '[]' in case of downgrading FW such as 'nvmupdate:'
#  i40e: []     # remove '[]' in case of downgrading FW to get required version of NVM 'i40e' 700 Series such as 'i40e:'
#     nvmupdate_pkg_url: "https://downloadmirror.intel.com/739639/700Series_NVMUpdatePackage_v9_00_Linux.tar.gz"
#     nvmupdate_pkg_checksum: "sha1:B2B183ADD3B6EF8BCB2DA77A6E6A68F482F4BFD1"
#     required_fw_version: "9.0"
#      # min fw version for ddp was taken from:
#      # https://www.intel.com/content/www/us/en/developer/articles/technical/dynamic-device-personalization-for-intel-ethernet-700-series.html
#     min_ddp_loadable_fw_version: "6.01"
#     min_updatable_fw_version: "5.02"
#      # when downgrading only, the recommended below version is required to download the supported NVMupdate64E tool. Users should replace the tool at their own risk.
#     supported_nvmupdate_tool_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#     supported_nvmupdate_tool_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#     supported_nvmupdate_tool_fw_version: "4.0"
{%- endif %}
{%- if nic == 'cvl' %}
#nvmupdate: []  # remove '[]' in case of downgrading FW such as 'nvmupdate:'
#  ice: []      # remove '[]' in case of downgrading FW to get required version of NVM 'ICE' 800 Series such as 'ice:'
#     nvmupdate_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#     nvmupdate_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#     required_fw_version: "4.0"
#      # https://builders.intel.com/docs/networkbuilders/intel-ethernet-controller-800-series-device-personalization-ddp-for-telecommunications-workloads-technology-guide.pdf
#      # document above does not specify any min fw version needed for ddp feature. So, min_ddp_loadable_fw is the same as min_updatable_fw
#     min_ddp_loadable_fw_version: "0.70"
#     min_updatable_fw_version: "0.70"
       # when downgrading only, the recommended below version is required to download the supported NVMupdate64E tool. Users should replace the tool at their own risk.
#     supported_nvmupdate_tool_pkg_url: "https://downloadmirror.intel.com/738715/E810_NVMUpdatePackage_v4_00_Linux.tar.gz"
#     supported_nvmupdate_tool_pkg_checksum: "sha1:7C168880082653B579FDF225A2E6E9301C154DD1"
#     supported_nvmupdate_tool_fw_version: "4.0"
{%- endif %}
{% if ddp in ['on', 'optional'] %}
# install Intel x700 & x800 series NICs DDP packages
install_ddp_packages: {% if ddp == 'on' and nic == 'fvl'%}true{% else %}false{% endif %}
# If following error appears: "Flashing failed: Operation not permitted"
# run deployment with update_nic_firmware: true
# or
# Disable ddp installation via install_ddp_packages: false

# set 'true' to enable custom ddp package to be loaded after reboot
enable_ice_systemd_service: {% if ddp == "on" %}true{% else %}false{% endif %}
{% endif %}

{%- if sriov_network_dp in ['on', 'optional'] %}
sriov_cni_enabled: {% if sriov_network_dp == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if sriov_operator in ['on', 'optional'] %}
# Custom SriovNetworkNodePolicy manifests local path
# custom_sriov_network_policies_dir: /tmp/sriov
{%- endif %}

{%- if bond_cni in ['on', 'optional'] %}
# Bond CNI
bond_cni_enabled: {% if bond_cni == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if dpdk in ['on', 'optional'] %}
# Install DPDK (required for SR-IOV networking)
install_dpdk: {% if dpdk == 'on' %}true{% else %}false{% endif %}
# DPDK version (will be in action if install_dpdk: true)
dpdk_version: {% if intel_flexran == 'on' %}"21.11"{% else %}"22.07"{% endif %}
# Custom DPDK patches local path
{% if intel_flexran == 'on' %}dpdk_local_patches_dir: "/tmp/flexran"{% else %}#dpdk_local_patches_dir: "/tmp/patches/dpdk"{% endif %}
# It might be necessary to adjust the patch strip parameter, update as required.
{% if intel_flexran == 'on' %}dpdk_local_patches_strip: 1{% else %}#dpdk_local_patches_strip: 0{% endif %}
{%- endif %}
{% if network_userspace in ['on', 'optional'] %}
# Userspace networking
userspace_cni_enabled: {% if network_userspace == 'on' %}true{% else %}false{% endif %}
ovs_dpdk_enabled: {% if ovs_dpdk == 'on' %}true{% else %}false{% endif %} # Should be enabled with Userspace CNI, when VPP is set to "false"; 1G hugepages required
ovs_version: "v2.17.2"
# CPU mask for OVS-DPDK PMD threads
ovs_dpdk_lcore_mask: 0x1
# Huge memory pages allocated by OVS-DPDK per NUMA node in megabytes
# example 1: "256,512" will allocate 256MB from node 0 and 512MB from node 1
# example 2: "1024" will allocate 1GB from node 0 on a single socket board, e.g. in a VM
ovs_dpdk_socket_mem: "256,0"
vpp_enabled: {% if vpp == 'on'%}true{% else %}false{% endif %} # Should be enabled with Userspace CNI, when ovs_dpdk is set to "false"; 2M hugepages required
{% endif %}

{%- if hugepages in ['on', 'optional'] %}
# Enables hugepages support
hugepages_enabled: {% if hugepages == 'on' %}true{% else %}false{% endif %}
# Hugepage sizes available: 2M, 1G
default_hugepage_size: {% if vpp == 'on' %}2M{% else %}1G{% endif %}
# Sets how many hugepages should be created
number_of_hugepages_1G: 4
number_of_hugepages_2M: 1024
{% endif %}
{%- if dlb in ['on', 'optional'] and arch in ['spr'] %}
# Configure SIOV and Intel DLB devices - required for Intel DLB Device Plugin support
configure_dlb_devices: {% if dlb == "on" %}true{% else %}false{% endif %}
{% endif %}

{%- if dsa in ['on', 'optional'] and arch in ['spr'] %}
# Configure SIOV and Intel DSA devices - required for Intel DSA Device Plugin support
configure_dsa_devices: {% if dsa == "on" %}true{% else %}false{% endif %}

# Example DSA devices configuration list. If left empty and configure_dsa_devices is set to true then default configuration will be applied.
# It is possible to configure more DSA devices by extending dsa_devices list based on example config.
dsa_devices: []
  # - name: dsa0  # name of DSA device from /sys/bus/dsa/devices/
  #   groups: 1  # number of groups to configure. The maximum number of groups per device can be found on /sys/bus/dsa/devices/dsaX/max_groups
  #   engines: 1  # number of engines to configure - one engine per group will be configured.
  #               # The maximum number of engines can be found on /sys/bus/dsa/devices/dsa0/max_engines
  #   wqs:  # work queues will be named as wq<dsa_id>.<wq_id>, for example wq0.0 - WQ with id 0 owned by dsa0 device
  #     - id: 0  # work queue id
  #       mode: "dedicated"  # [shared, dedicated]
  #       type: "user"  # [kernel, user]
  #       size: 8  # sum of all configured WQs size must be less than /sys/bus/dsa/devices/dsa0/max_workqueue_size
  #       prio: 4  # must be set between 1 and 15
  #       group_id: 0  # work queue will be assigned to specific group
  #       max_batch_size: 1024  # specify the max batch size used by a work queue - powers of 2 are accetable
  #       max_transfer_size: 2147483648  # specify the max transfer size used by a work queue - powers of 2 are accetable
  #       block_on_fault: 0  # [0, 1] If block on fault is disabled,
  #                          # if a page fault occurs on a source or destination memory access, the operation stops and the page fault is reported to the software
  #     - id: 1
  #       mode: "shared"
  #       type: "user"
  #       size: 8
  #       prio: 5
  #       threshold: 7  # only for Shared WQ, must be at least one less than size of WQ
  #       group_id: 0
  #       max_batch_size: 1024
  #       max_transfer_size: 2147483648
  #       block_on_fault: 0
{% endif %}

{%- if intel_ethernet_operator.enabled in ['on', 'optional'] %}
# Intel Ethernet Operator for Intel E810 Series network interface cards
intel_ethernet_operator:
{%- if intel_ethernet_operator.ddp in ['on', 'optional'] %}
  ddp_update: {% if intel_ethernet_operator.ddp == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}                   # perform DDP update on PFs listed in dataplane_interfaces using selected DDP profile
{%- endif %}
  fw_update: {% if intel_ethernet_operator.fw_update == 'on' and nic == 'cvl' %}true{% else %}false{% endif %}                     # perform firmware update on PFs listed in dataplane_interfaces
  # NodeFlowConfig manifests local path
  # For more information refer to:
  # https://github.com/intel/intel-ethernet-operator/blob/main/docs/flowconfig-daemon/creating-rules.md
  # node_flow_config_dir: /tmp/node_flow_config
{% endif %}

{%- if intel_sriov_fec_operator in ['on', 'optional'] %}
# Wireless FEC H/W Accelerator Device (e.g. ACC100) PCI ID
fec_acc: "0000:6f:00.0" # must be string in [a-fA-F0-9]{4}:[a-fA-F0-9]{2}:[01][a-fA-F0-9].[0-7] format
{% endif %}

{%- if intel_flexran in ['on', 'optional'] %}
# Intel FlexRAN
intel_flexran_enabled: {% if intel_flexran == 'on' %}true{% else %}false{% endif %} # if true, deploy FlexRAN
{% endif %}

{%- if qat in ['on', 'optional'] %}
# Enabling this feature will install QAT drivers + services
update_qat_drivers: {% if qat == "on" %}true{% else %}false{% endif %}

{%- if arch in ['spr'] %}
# SPR platform QAT drivers requirements
# the folder must exist on the machine that will run Ansible playbooks
qat_drivers_folder: "/tmp/qat/" # QAT "QAT20.L.0.9.6-00024.tar.gz" driver package is expected to be present in this folder
qat_drivers_version: "QAT20.L.0.9.6-00024" # CEK has been validated with QAT drivers version "QAT20.L.0.9.6-00024.tar.gz"
# This package provides user space libraries that allow access to Intel(R) QuickAssist devices and expose the Intel(R) QuickAssist APIs and sample codes.
enable_intel_qatlibs: {% if qat == "on" %}true{% else %}false{% endif %} # Make sure "openssl_install" is set to "true" else this feature will be skipped in deployment.
enable_qat_svm: {% if qat == "on" %}true{% else %}false{% endif %} # Enable QAT Shared Virtual Memory (SVM).
{% endif %}
# qat interface configuration list
qat_devices: []
{%- if on_vms == 'on' %}
#  - qat_id: "0000:0a:00.0"
#    qat_sriov_numvfs: 0              # Have to be set to 0 here to not create any VFs inside VM.

#  - qat_id: "0000:0b:00.0"
#    qat_sriov_numvfs: 0              # Have to be set to 0 here to not create any VFs inside VM.
{%- else %}
#  - qat_id: "0000:ab:00.0"            # QAT device id one using DPDK compatible driver for VF devices to be used by vfio-pci kernel driver, replace as required
#    qat_sriov_numvfs: 12              # Number of VFs per PF to create - cannot exceed the maximum number of VFs available for the device. Set to 0 to not create any VFs.
#                                      # Note: Currently when trying to create fewer virtual functions than the maximum, the maximum number always gets created.
#    qat_default_vf_driver: {% if arch == "spr" %}"4xxxvf"{% else %}"c6xxvf"{% endif %}
#    qat_vfs:                          # VFs drivers settings will be overridden by QAT device plugin.
#      vf_00: "vfio-pci"
#      vf_05: "vfio-pci"

#  - qat_id: "0000:xy:00.0"
#    qat_sriov_numvfs: 10
#    qat_default_vf_driver: {% if arch == "spr" %}"4xxxvf"{% else %}"c6xxvf"{% endif %}
#    qat_vfs: {}

#  - qat_id: "0000:yz:00.0"
#    qat_sriov_numvfs: 10
#    qat_default_vf_driver: {% if arch == "spr" %}"4xxxvf"{% else %}"c6xxvf"{% endif %}
#    qat_vfs: {}
{%- endif %}
{% endif %}

{%- if openssl in ['on', 'optional'] %}
# Install and configure OpenSSL cryptography
openssl_install: {% if openssl == 'on' and qat == "on" %}true{% else %}false{% endif %} # This requires update_qat_drivers set to 'true' in host vars
{% endif -%}
{%- if isolcpu in ["on", "optional"] %}
# CPU isolation from Linux scheduler
isolcpus_enabled: {% if isolcpu == 'on' %}true{% else %}false{% endif %}
{%- if on_vms == 'on' %}
isolcpus: "4-15"
{%- else -%}
{% if vm_mode == 'on' %}
# isolcpus variable can't be enabled in case of VMRA deployment.
# Its content is generated automatically.
# isolcpus: ""
{%- else %}
isolcpus: "4-11"
{% endif %}
{%- endif %}
{%- endif %}
{%- if cpusets in ["on", "optional"] %}
# CPU shielding
cpusets_enabled: {% if cpusets == 'on' %}true{% else %}false{% endif %}
{%- if on_vms == 'on' %}
cpusets: "4-15"
{%- else %}
cpusets: "4-11"
{%- endif %}
{% endif %}


{%- if native_cpu_manager in ["on", "optional"] %}
# Native CPU Manager (Kubernetes built-in)
# These settings are relevant only if in group_vars native_cpu_manager_enabled: true
# Amount of CPU cores that will be reserved for the housekeeping (2000m = 2000 millicores = 2 cores)
native_cpu_manager_system_reserved_cpus: 2000m
# Amount of CPU cores that will be reserved for Kubelet
native_cpu_manager_kube_reserved_cpus: 1000m
# Explicit list of the CPUs reserved for the host level system threads and Kubernetes related threads
#native_cpu_manager_reserved_cpus: "0,1,2"
# Note: All remaining unreserved CPU cores will be consumed by the workloads.
{% endif %}
{%- if (pstate in ['on', 'optional'] or sst in ['on', 'optional']) and arch in ['clx', 'icx', 'spr'] %}
# Enable/Disable Intel PState scaling driver
intel_pstate_enabled: {% if pstate == "on" or sst == "on" %}true{% else %}false{% endif %}
# Config options for intel_pstate: disable, passive, force, no_hwp, hwp_only, support_acpi_ppc, per_cpu_perf_limits
intel_pstate: {% if pstate == "on" or sst == "on" %}hwp_only{% else %}disable{% endif %}
# Enable/Disable Intel Turbo Boost PState attribute
turbo_boost_enabled: {% if on_vms != 'on' %}true{% else %}false{% endif %}
{% endif -%}

{% if cstate in ['on', 'optional'] %}
cstate_enabled: {% if cstate == "on" %}true{% else %}false{% endif %}
cstates:
{%- if name == 'access' %}
  C6: # default values: C6 for access, C1 for other profiles
    cpu_range: '0-9' # change as needed, cpus to modify cstates on
    enable: true # true - enable given cstate, false - disable given cstate
{%- else %}
  C1: # default values: C6 for access, C1 for other profiles
    cpu_range: '0-9' # change as needed, cpus to modify cstates on
    enable: true # true - enable given cstate, false - disable given cstate
{% endif -%}
{% endif -%}

{% if ufs in ['on', 'optional'] %}
ufs_enabled: {% if ufs == "on" %}true{% else %}false{% endif %}
ufs: # uncore frequency scaling
  min: 1000 # minimal uncore frequency
  max: 2000 # maximal uncore frequency
{% endif -%}

{% if sst in ['on', 'optional'] %}
{%- if arch in ['icx', 'spr'] %}
# Intel(R) SST-PP (perf-profile) configuration
# [true] Enable Intel(R) SST-PP (perf-profile)
# [false] Disable Intel(R) SST-PP (perf-profile)
sst_pp_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
sst_pp_config_list:             # "enable" or "disable" list options per SST-PP setup requirements
    - sst_bf: "enable"          # "enable" or "disable" Intel(R) SST-BF (base-freq) to configure with SST-PP
    - sst_cp: "enable"          # "enable" or "disable" Intel(R) SST-CP (core-power) to configure with SST-PP.
    - sst_tf: "enable"          # "enable" or "disable" Intel(R) SST-TF (turbo-freq) to configure with SST-PP.
      online_cpus_range: "auto" # "auto" will config turbo-freq for all available online CPUs or else define specific CPUs such as "2,3,5" to prioritize among others.
{% endif %}
# Intel Speed Select Base-Frequency configuration.
{%- if arch == 'clx' %}
sst_bf_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
# Intel Speed Select Base-Frequency configuration for Cascade Lake (CLX) Platforms.
# CLX support of SST-BF requires 'intel_pstate' to be 'enabled'
# Option clx_sst_bf_mode requires sst_bf_configuration_enabled to be set to 'true'.
# There are three configuration modes:
# [s] Set SST-BF config (set min/max to 2700/2700 and 2100/2100)
# [m] Set P1 on all cores (set min/max to 2300/2300)
# [r] Revert cores to min/Turbo (set min/max to 800/3900)
clx_sst_bf_mode: s
{%- endif %}
{%- if arch == 'icx' %}
# Intel Speed Select Base-Frequency configuration for Ice Lake (ICX) Platforms.
# [true] Enable Intel Speed Select Base Frequency (SST-BF)
# [false] Disable Intel Speed Select Base Frequency (SST-BF)
# Requires `sst_bf_configuration_enabled` variable to be 'true'
icx_sst_bf_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
# Prioritze (SST-CP) power flow to high frequency cores in case of CPU power constraints.
icx_sst_bf_with_core_priority: {% if sst == "on" %}true{% else %}false{% endif %}

# SST CP config
# Variables are only examples.
# For more information, please visit:
# https://www.kernel.org/doc/html/latest/admin-guide/pm/intel-speed-select.html#enable-clos-based-prioritization
# Enabling this configuration overrides `icx_sst_bf_with_core_priority`.
sst_cp_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
sst_cp_priority_type: "1" # For Proportional select "0" and for Ordered select "1", update as required
sst_cp_clos_groups: # configure up to 4 CLOS groups
  - id: 0
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 1
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 2
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
  - id: 3
    frequency_weight: 0 # used only with Proportional type
    min_MHz: 0
    max_MHz: 25500
sst_cp_cpu_clos: # assign required values to CLOS group after priority type setup
  - clos: 0      # associating CPUs with a CLOS group such as "clos: 3", update as required.
    cpus: 2,3,5  # define specific CPUs per CLOS group such as "cpus: 2,6,9", update as required.
  - clos: 1
    cpus: 12

# Intel(R) SST-TF (feature turbo-freq) configuration for Ice Lake (ICX) Platforms.
# [true] Enable Intel Speed Select Turbo Frequency (SST-TF)
# [false] Disable Intel Speed Select Turbo Frequency (SST-TF)
sst_tf_configuration_enabled: {% if sst == "on" %}true{% else %}false{% endif %}
{%- endif %}
{% endif %}

{%- if sgx in ['on', 'optional'] and arch in ['icx'] %}
# Intel Software Guard Extensions (SGX)
configure_sgx: {% if sgx == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if gpu in ['on', 'optional'] %}
# Intel custom GPU kernel - this is required to be true in order to
# deploy Intel GPU Device Plugin on that node
configure_gpu: {% if gpu == 'on' %}true{% else %}false{% endif %}
{% endif %}

{%- if telemetry.collectd in ['on', 'optional'] %}
# Telemetry configuration
# intel_pmu plugin collects information provided by Linux perf interface.
enable_intel_pmu_plugin: false

{%- if on_vms == 'on' %}

# Temporary Fix for collectd startup issue on VMs
enable_pkgpower_plugin: false
{%- endif %}

# CPU Threads to be monitored by Intel PMU Plugin.
# If the field is empty, all available cores will be monitored.
# Please refer to https://collectd.org/wiki/index.php/Plugin:Intel_PMU for configuration details.
intel_pmu_plugin_monitored_cores: ""

# CPU Threads to be monitored by Intel RDT Plugin.
# If the field is empty, all available cores will be monitored.
# Please refer to https://collectd.org/wiki/index.php/Plugin:IntelRDT for configuration details.
intel_rdt_plugin_monitored_cores: ""

# Additional list of plugins that will be excluded from collectd deployment.
exclude_collectd_plugins: []
{% endif %}

{%- if cndp in ['on', 'optional'] %}
# Intel Cloud Native Data Plane.
cndp_enabled: {% if cndp == 'on' %}true{% else %}false{% endif %}
{%- if cndp_dp in ['on', 'optional'] %}
cndp_dp_pools:
  - name: "e2e"
    drivers: {% raw %}"{{ dataplane_interfaces | map(attribute='pf_driver') | list | unique }}"{% endraw %}   # List of NIC driver to be included in CNDP device plugin ConfigMap.
{% endif %}
{% endif %}

{% if adq_dp in ['on', 'optional'] %}
# Note: ADQ is experimental feature and enabling it may lead to unexpected results.
# ADQ requires back-to-back connection between control plane and worker node on CVL interfaces.
# Name of CVL interfaces must be the same on both nodes, IP address must be present.
# In inventory.ini set "ip=" to IP address of CVL interface
adq_dp:
  enabled: false
  # IP address of CVL interface located on the worker node
  interface_address: "192.168.0.11"
{% endif %}

{%- if vm_mode in ['on'] and on_vms != 'on' %}
# The only common VM image for all VMs inside deployment is supported at the moment
#
{%- if secondary_host == 'true' %}
# Do not set VM image info here - do it just on the first vm_host
# Secondary vm_host - do not change dhcp settings here
dhcp: []
{% else %}
# Default VM image version is Ubuntu 20.04 - focal
# Supported VM image distributions ['ubuntu', 'rocky']. Default is 'ubuntu'.
#vm_image_distribution: "rocky"
# Supported VM image ubuntu versions ['20.04', '22.04']. Default version is '20.04'.
#vm_image_version_ubuntu: "22.04"
# Supported VM image rocky versions ['8.5', '9.0']. Default version is '8.5'.
#vm_image_version_rocky: "9.0"
# dhcp for vxlan have to be enabled just on the first vm_host
dhcp:
  - 120
#vxlan_gw_ip: "40.0.0.1/24"
{% endif -%}
# Set hashed password for root user inside VMs. Current value is just placeholder.
# To create hashed password use e.g.: openssl passwd -6 -salt SaltSalt <your_password>
vm_hashed_passwd: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
vxlan_device: eno2
#cpu_host_os will change number of CPUs reserved for host OS. Default value is 16
#cpu_host_os: 8
vms:
{%- if secondary_host == 'true' %}
#  - type: "ctrl"
#    name: "vm-ctrl-1"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
{% else %}
  - type: "ctrl"
    name: "vm-ctrl-1"
    # Parameter cpus and numa can be uncommented and specified manually, but by default
    # are CPUs and NUMA node allocated automatically.
    # cpus: "8-11,64-67"
    # numa: 0
    # Parameter emu_cpus is DEPRECATED, uncommenting line below would not have
    # any impact, as emulators CPUs are picked-up automatically.
    # emu_cpus: "8,64"
    # if you set cpu_total: to 0 then rest of unallocated CPUs from selected numa will be used
    cpu_total: 8
    # if 'alloc_all: true' is used then 'cpu_total' have to be set to '0'
    # It will take all unallocated CPUs from all NUMA nodes on the vm_host.
    # alloc_all: true
    memory: 20480
    vxlan: 120
{% endif -%}
#  - type: "ctrl"
#    name: "vm-ctrl-2"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
#  - type: "ctrl"
#    name: "vm-ctrl-3"
#    cpu_total: 8
#    memory: 20480
#    vxlan: 120
{%- if secondary_host == 'true' %}
#  - type: "work"
#    name: "vm-work-1"
#    cpu_total: 16
#    memory: 61440
#    vxlan: 120
{%- if name not in ['build_your_own'] %}
#    pci:
#      - "18:02.2"
#      - "18:02.3"
#      - "18:02.4"
#      - "18:02.5"
{%- if qat == "on" %}
##      - "3d:01.1"
##      - "3f:01.1"
{%- endif %}
{%- else %}
#    pci: []
{%- endif %}
{% else %}
  - type: "work"
    name: "vm-work-1"
    # Parameter cpus and numa can be uncommented and specified manually, but by default
    # are CPUs and NUMA node allocated automatically.
    # cpus: "28-35,84-91"
    # numa: 1
    # Parameter emu_cpus is DEPRECATED, uncommenting line below would not have
    # any impact, as emulators CPUs are picked-up automatically.
    # emu_cpus: "28,84"
    # if you set cpu_total: to 0 then rest of unallocated CPUs from selected numa will be used
    cpu_total: 16
    # if 'alloc_all: true' is used then 'cpu_total' have to be set to '0'
    # It will take all unallocated CPUs from all NUMA nodes on the vm_host.
    # alloc_all: true
    memory: 61440
    vxlan: 120
{%- if name not in ['build_your_own'] %}
    pci:
      - "18:02.2"
      - "18:02.3"
      - "18:02.4"
      - "18:02.5"
{%- if qat == "on" %}
#      - "3d:01.1"
#      - "3f:01.1"
{%- endif %}
{%- else %}
    pci: []
{%- endif %}
{% endif -%}
#  - type: "work"
#    name: "vm-work-2"
#    cpu_total: 16
#    memory: 61440
#    vxlan: 120
{%- if name not in ['build_your_own'] %}
#    pci:
#      - "18:02.0"
#      - "18:02.1"
#      - "18:02.6"
#      - "18:02.7"
{%- if qat == "on" %}
##      - "3d:01.2"
##      - "3f:01.2"
{%- endif %}
{%- else %}
#    pci: []
{%- endif %}
{% endif -%}
{%- if power_manager in ['on', 'optional'] and arch in ['icx', 'clx', 'spr'] -%}
# Power Manager Shared Profile/Workload settings.
# It is possible to create node-specific Power Profile
local_shared_profile:
  enabled: false
  node_max_shared_frequency: 2000
  node_min_shared_frequency: 1500

# Shared Workload is required to make use of Shared Power Profile
shared_workload:
  enabled: false
  reserved_cpus: []   # The CPUs in reserved_cpus should match the value of the reserved system CPUs in your Kubelet config file, if none please
                      # set here a dummy core - the last one to avoid AppQos bug
  shared_workload_type: "global"  # set to node name to make use of node-specific Power Profile, 'global' means use cluster-specific custom Power Profile
{% endif %}
{%- if minio in ['on', 'optional'] %}
# MinIO storage configuration
minio_pv: []
#  - name: "mnt-data-1"                         # PV identifier will be used for PVs names followed by node name(e.g., mnt-data-1-hostname)
#    storageClassName: "local-storage"          # Storage class name to match with PVC
#    accessMode: "ReadWriteOnce"                # Access mode when mounting a volume, e.g., ReadWriteOnce/ReadOnlyMany/ReadWriteMany/ReadWriteOncePod
#    persistentVolumeReclaimPolicy: "Retain"    # Reclaim policy when a volume is released once it's bound, e.g., Retain/Recycle/Delete
#    mountPath: /mnt/data0                      # Mount path of a volume
#    device: /dev/nvme0n1                       # Target storage device name when creating a volume.
                                                # When group_vars: minio_deploy_test_mode == true, use a file as a loop device for storage
                                                # otherwise, an actual NVME or SSD device for storage on the device name.

#  - name: "mnt-data-2"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    mountPath: /mnt/data1
#    device: /dev/nvme1n1

#  - name: "mnt-data-3"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    mountPath: /mnt/data2
#    device: /dev/nvme2n1

#  - name: "mnt-data-4"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    mountPath: /mnt/data3
#    device: /dev/nvme3n1

#  - name: "mnt-data-5"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    mountPath: /mnt/data4
#    device: /dev/nvme4n1

#  - name: "mnt-data-6"
#    storageClassName: "local-storage"
#    accessMode: "ReadWriteOnce"
#    persistentVolumeReclaimPolicy: "Retain"
#    mountPath: /mnt/data5
#    device: /dev/nvme5n1
{% endif -%}
{% else %}
dhcp:
  - 120

# Set hashed password for root user inside VMs. Current value is just placeholder.
# To create hashed password use e.g.: openssl passwd -6 -salt SaltSalt <your_password>
vm_hashed_passwd: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
vxlan_device: eno2

vms:
  - name: "vm-ctrl-1"
    image_distribution: "ubuntu"
    cpu_total: 2
    memory: 4096
    vxlan: 120
    type: "ctrl"
  - name: "vm-work-1"
    image_distribution: "windows"
    cpu_total: 2
    memory: 4096
    vxlan: 120
    type: "work"
{% endif %}
